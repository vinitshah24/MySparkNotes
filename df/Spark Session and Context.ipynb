{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef49468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb64eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf16fa",
   "metadata": {},
   "source": [
    "## DIfferent Spark Session but SAME Spark Context\n",
    "* The newSession method creates a new spark session with isolated SQL configurations, temporary tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aff801",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "* A SparkContext represents the connection to a Spark cluster.\n",
    "* It is used to create RDDs, accumulators and broadcast variables on that cluster. \n",
    "* ***Note: Only one SparkContext should be active per JVM. You must stop() the active SparkContext before creating a new one.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa77e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001ABA30F51F0>\n",
      "<SparkContext master=local[*] appName=SparkSQL>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSQL\").getOrCreate()\n",
    "print(spark)\n",
    "print(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e920c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.memory\", '8g')\n",
    "# spark.conf.set('spark.executor.cores', '3')\n",
    "# spark.conf.set('spark.cores.max', '3')\n",
    "# spark.conf.set(\"spark.driver.memory\",'8g')\n",
    "spark.conf.set(\"VAR1\", \"TEST\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8a1658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"VAR1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685820ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conf ===\n",
      "('spark.app.submitTime', '1681791418690')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.name', 'SparkSQL')\n",
      "('spark.app.startTime', '1681791418989')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.sql.warehouse.dir', 'file:/D:/BigData/Spark/spark-warehouse')\n",
      "('spark.driver.host', 'DESKTOP-SSC2TF1')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.driver.port', '56631')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1681791420788')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Conf ===\")\n",
    "confList = spark.sparkContext.getConf().getAll()\n",
    "# confList = spark.sparkContext._conf.getAll()\n",
    "for conf in confList:\n",
    "    print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2aa908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+------+\n",
      "|  first| last|birth_date|salary|\n",
      "+-------+-----+----------+------+\n",
      "|  James|Smith|1998-04-01|  3000|\n",
      "|Michael| Rose|2000-05-19|  4000|\n",
      "|  Maria|Jones|1999-12-01|  4000|\n",
      "+-------+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 'Smith', '1998-04-01', 3000),\n",
    "        ('Michael', 'Rose', '2000-05-19', 4000),\n",
    "        ('Maria', 'Jones', '1999-12-01', 4000)]\n",
    "columns = [\"first\", \"last\", \"birth_date\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.createOrReplaceTempView(\"employee_tmp\")\n",
    "spark.sql(\"SELECT * FROM employee_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f39f5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.tableExists('employee_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4c8ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_tmp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ecf91",
   "metadata": {},
   "source": [
    "## newSession()\n",
    "* Configurations set in Spark Session are not shared.\n",
    "* Temporary tables are not shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ef3cc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001ABA42234F0>\n",
      "<SparkContext master=local[*] appName=SparkSQL>\n"
     ]
    }
   ],
   "source": [
    "sparkNew = spark.newSession()\n",
    "print(sparkNew)\n",
    "print(sparkNew.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "031577ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conf ===\n",
      "('spark.app.submitTime', '1681791418690')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.name', 'SparkSQL')\n",
      "('spark.app.startTime', '1681791418989')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.sql.warehouse.dir', 'file:/D:/BigData/Spark/spark-warehouse')\n",
      "('spark.driver.host', 'DESKTOP-SSC2TF1')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.driver.port', '56631')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1681791420788')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Conf ===\")\n",
    "confList = spark.sparkContext.getConf().getAll()\n",
    "for conf in confList:\n",
    "    print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be215157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o157.get.\n",
      ": java.util.NoSuchElementException: VAR1\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:2138)\r\n",
      "\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5032)\r\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\r\n",
      "\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5032)\r\n",
      "\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sparkNew.conf.get(\"VAR1\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91ba03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkNew.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6cddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkNew.catalog.tableExists('employee_tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337dfbd",
   "metadata": {},
   "source": [
    "## New Session using getOrCreate()\n",
    "* Here existing SparkSession was used so spark configurations and temporary tables are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bc7e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001ABA30F51F0>\n",
      "<SparkContext master=local[*] appName=SparkSQL>\n"
     ]
    }
   ],
   "source": [
    "spark2 = SparkSession.builder.master(\"local[*]\").appName(\"SparkSQL\").getOrCreate()\n",
    "print(spark2)\n",
    "print(spark2.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c42df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark2.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d3ec056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark2.catalog.tableExists('employee_tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
