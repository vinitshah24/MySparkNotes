{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `skewed dataset` is defined by a dataset that has a class imbalance, this leads to poor or failing spark jobs that often get a `OOM` (out of memory) error.\n",
    "\n",
    "When performing a `join` onto a `skewed dataset` it's usually the case where there is an imbalance on the `key`(s) on which the join is performed on. This results in a majority of the data falls onto a single partition, which will take longer to complete than the other partitions.\n",
    "\n",
    "Some hints to detect skewness is:\n",
    "1. The `key`(s) consist mainly of `null` values which fall onto a single partition.\n",
    "2. There is a subset of values for the `key`(s) that makeup the high percentage of the total keys which fall onto a single partition.\n",
    "\n",
    "We go through both these cases and see how we can combat it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"Exploring Joins\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situation 1: Null Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inital Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|card_id|\n",
      "+---+-------+\n",
      "|  1|   null|\n",
      "|  2|   null|\n",
      "|  3|      1|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers = spark.createDataFrame([\n",
    "    (1, None), \n",
    "    (2, None), \n",
    "    (3, 1),\n",
    "], [\"id\", \"card_id\"])\n",
    "\n",
    "customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+\n",
      "|card_id|first_name|last_name|age|\n",
      "+-------+----------+---------+---+\n",
      "|      1|      john|      doe| 21|\n",
      "|      2|      rick|     roll| 10|\n",
      "|      3|       bob|    brown|  2|\n",
      "+-------+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cards = spark.createDataFrame([\n",
    "    (1, \"john\", \"doe\", 21), \n",
    "    (2, \"rick\", \"roll\", 10), \n",
    "    (3, \"bob\", \"brown\", 2)\n",
    "], [\"card_id\", \"first_name\", \"last_name\", \"age\"])\n",
    "\n",
    "cards.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #1: Join Regularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+---------+----+\n",
      "|card_id| id|first_name|last_name| age|\n",
      "+-------+---+----------+---------+----+\n",
      "|   null|  1|      null|     null|null|\n",
      "|   null|  2|      null|     null|null|\n",
      "|      1|  3|      john|      doe|  21|\n",
      "+-------+---+----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = customers.join(cards, \"card_id\", \"left\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [card_id#1L, id#0L, first_name#14, last_name#15, age#16L]\n",
      "   +- SortMergeJoin [card_id#1L], [card_id#13L], LeftOuter\n",
      "      :- Sort [card_id#1L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(card_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=164]\n",
      "      :     +- Scan ExistingRDD[id#0L,card_id#1L]\n",
      "      +- Sort [card_id#13L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(card_id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=165]\n",
      "            +- Filter isnotnull(card_id#13L)\n",
      "               +- Scan ExistingRDD[card_id#13L,first_name#14,last_name#15,age#16L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+---------+---+\n",
      "|card_id| id|first_name|last_name|age|\n",
      "+-------+---+----------+---------+---+\n",
      "|      1|  3|      john|      doe| 21|\n",
      "+-------+---+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = customers.join(cards, \"card_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [card_id#1L, id#0L, first_name#14, last_name#15, age#16L]\n",
      "   +- SortMergeJoin [card_id#1L], [card_id#13L], Inner\n",
      "      :- Sort [card_id#1L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(card_id#1L, 200), ENSURE_REQUIREMENTS, [plan_id=305]\n",
      "      :     +- Filter isnotnull(card_id#1L)\n",
      "      :        +- Scan ExistingRDD[id#0L,card_id#1L]\n",
      "      +- Sort [card_id#13L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(card_id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=306]\n",
      "            +- Filter isnotnull(card_id#13L)\n",
      "               +- Scan ExistingRDD[card_id#13L,first_name#14,last_name#15,age#16L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened**:\n",
    "* Rows that didn't join up were brought to the join.\n",
    "\n",
    "* For a `left join`, they will get `Null` values for the right side columns, what's the point of being them in?\n",
    "* For a `inner join`, they rows will get dropped, so again what's the point of being them in?\n",
    "\n",
    "**Results**:\n",
    "* We brought more rows to the join than we had to. These rows get normally get put onto a single partition. \n",
    "* If the data is large enough and the percentage of keys that are null is high. The program could OOM out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #2: Filter Null Keys First, then Join, then Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+---------+----+\n",
      "| id|card_id|first_name|last_name| age|\n",
      "+---+-------+----------+---------+----+\n",
      "|  1|   null|      null|     null|null|\n",
      "|  2|   null|      null|     null|null|\n",
      "|  3|      1|      john|      doe|  21|\n",
      "+---+-------+----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def null_skew_helper(left, right, key):\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "        1. Filter out the null rows.\n",
    "        2. Create the columns you would get from the join.\n",
    "        3. Join the tables.\n",
    "        4. Union the null rows to joined table.\n",
    "    \"\"\"\n",
    "    lr_null_rows = left.where(F.col(key).isNull())\n",
    "    for f in right.schema.fields:\n",
    "            lr_null_rows = lr_null_rows.withColumn(f.name, F.lit(None).cast(f.dataType))\n",
    "    left_non_null_rows = left.where(F.col(key).isNotNull())\n",
    "    lr_non_null_rows = left_non_null_rows.join(right, key, \"left\")\n",
    "    return lr_null_rows.union(lr_non_null_rows.select(lr_null_rows.columns))\n",
    "    \n",
    "\n",
    "df = null_skew_helper(customers, cards, \"card_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Union\n",
      "   :- Project [id#0L, null AS card_id#90L, null AS first_name#93, null AS last_name#97, null AS age#102L]\n",
      "   :  +- Filter isnull(card_id#1L)\n",
      "   :     +- Scan ExistingRDD[id#0L,card_id#1L]\n",
      "   +- Project [id#118L, card_id#119L, first_name#14, last_name#15, age#16L]\n",
      "      +- SortMergeJoin [card_id#119L], [card_id#13L], LeftOuter\n",
      "         :- Sort [card_id#119L ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(card_id#119L, 200), ENSURE_REQUIREMENTS, [plan_id=581]\n",
      "         :     +- Filter isnotnull(card_id#119L)\n",
      "         :        +- Scan ExistingRDD[id#118L,card_id#119L]\n",
      "         +- Sort [card_id#13L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(card_id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=582]\n",
      "               +- Filter isnotnull(card_id#13L)\n",
      "                  +- Scan ExistingRDD[card_id#13L,first_name#14,last_name#15,age#16L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened**:\n",
    "* We seperated the data into 2 sets:\n",
    "  * one where the `key`s are not `null`.\n",
    "  * one where the `key`s are `null`.\n",
    "* We perform the join on the set where the keys are not null, then union it back with the set where the keys are null. (This step is not necessary when doing an inner join).\n",
    "\n",
    "**Results**:\n",
    "* We brought less data to the join.\n",
    "* We read the data twice; more time was spent on reading data from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #3: Cache the Table, Filter Null Keys First, then Join, then Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_skew_helper(left, right, key):\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "        1. Cache table.\n",
    "        2. Filter out the null rows.\n",
    "        3. Create the columns you would get from the join.\n",
    "        4. Join the tables.\n",
    "        5. Union the null rows to joined table.\n",
    "    \"\"\"\n",
    "    left = left.cache()\n",
    "    df1 = left.where(F.col(key).isNull())\n",
    "    for f in right.schema.fields:\n",
    "            df1 = df1.withColumn(f.name, F.lit(None).cast(f.dataType))\n",
    "    df2 = left.where(F.col(key).isNotNull())\n",
    "    df2 = df2.join(right, key, \"left\")\n",
    "    return df1.union(df2.select(df1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+---------+----+\n",
      "| id|card_id|first_name|last_name| age|\n",
      "+---+-------+----------+---------+----+\n",
      "|  1|   null|      null|     null|null|\n",
      "|  2|   null|      null|     null|null|\n",
      "|  3|      1|      john|      doe|  21|\n",
      "+---+-------+----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = null_skew_helper(customers, cards, \"card_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Union\n",
      "   :- Project [id#0L, null AS card_id#161L, null AS first_name#164, null AS last_name#168, null AS age#173L]\n",
      "   :  +- Filter isnull(card_id#1L)\n",
      "   :     +- InMemoryTableScan [card_id#1L, id#0L], [isnull(card_id#1L)]\n",
      "   :           +- InMemoryRelation [id#0L, card_id#1L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) Scan ExistingRDD[id#0L,card_id#1L]\n",
      "   +- Project [id#189L, card_id#190L, first_name#14, last_name#15, age#16L]\n",
      "      +- SortMergeJoin [card_id#190L], [card_id#13L], LeftOuter\n",
      "         :- Sort [card_id#190L ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(card_id#190L, 200), ENSURE_REQUIREMENTS, [plan_id=849]\n",
      "         :     +- Filter isnotnull(card_id#190L)\n",
      "         :        +- InMemoryTableScan [id#189L, card_id#190L], [isnotnull(card_id#190L)]\n",
      "         :              +- InMemoryRelation [id#189L, card_id#190L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :                    +- *(1) Scan ExistingRDD[id#0L,card_id#1L]\n",
      "         +- Sort [card_id#13L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(card_id#13L, 200), ENSURE_REQUIREMENTS, [plan_id=850]\n",
      "               +- Filter isnotnull(card_id#13L)\n",
      "                  +- Scan ExistingRDD[card_id#13L,first_name#14,last_name#15,age#16L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened**:\n",
    "* Similar to option #2, but we did a `InMemoryTableScan` instead of two reads of the data.\n",
    "\n",
    "**Results**:\n",
    "* We brought less data to the join.\n",
    "* We did 1 less read, but we used more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "All to say:\n",
    "* It's definitely better to bring less data to a join, so performing a filter for `null keys` before the join is definitely suggested.\n",
    "* For `left join`s:\n",
    "    * By doing a union, this will result in an extra read of data or memory usage.\n",
    "    * Decide what you can afford; the extra read vs memory usage and `cache` the table before the `filter`.\n",
    "\n",
    "Always check the spread the values for the `join key`, to detect if there's any skew and pre filters that can be performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
