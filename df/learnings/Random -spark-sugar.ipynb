{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataTypes in PySpark:**  \n",
    "\n",
    "RDD:\n",
    "* Records\n",
    "\n",
    "DataFrames:\n",
    "* Rows\n",
    "* Columns\n",
    "\n",
    "Groupby:\n",
    "* Groups\n",
    "\n",
    "DataSets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.withColumn in PySpark:\n",
    "When you are do a `df.`**`withColumn()`** you cannot use the output of that expression in the same chained transform.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    "```\n",
    "```python\n",
    "# what not to do\n",
    ">>> df.withColumn('age', df['age']*2)\\\n",
    "...   .withColumn('new_age', df['age'] + 1)\\\n",
    "...   .collect()\n",
    "[\n",
    "    Row(struct=Row(age=4, new_age=3, name=u'Alice')), \n",
    "    Row(struct=Row(age=10, new_age=6, name=u'Bob'))\n",
    "]     \n",
    "# what to do\n",
    ">>> df = df.withColumn('age', df['age']*2)\n",
    "... df.withColumn('new_age', df['age'] + 1)\\\n",
    "...   .collect()\n",
    "[\n",
    "    Row(struct=Row(age=4, new_age=5, name=u'Alice')), \n",
    "    Row(struct=Row(age=10, new_age=11, name=u'Bob'))\n",
    "]     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins/Unions in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins:\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    ">>> df_2 = sc.sql.createDataFrame([('Alice', 'female'), ('Bob', 'male')], ['name','gender'])\n",
    ">>> df_3 = sc.sql.createDataFrame([('female', 'pink'), ('male', 'blue')], ['gender','color'])\n",
    "```\n",
    "\n",
    "*correctly joining DFs*  \n",
    "When not done correctly you will get duplicated columns.\n",
    "```python\n",
    "# what not to do\n",
    ">>> df_1.join(df_2, df_1['name'] == df_2['name'])\\\n",
    "...     .collect()\n",
    "[\n",
    "    Row(struct=Row(name=u'Alice', age=2, name=u'Alice', gender=u'female')), \n",
    "    Row(struct=Row(name=u'Bob', age=5, name=u'Bob', gender=u'male'))\n",
    "]\n",
    "# what to do\n",
    ">>> df_1.join(df_2, 'name')\\\n",
    "...     .collect()\n",
    "[\n",
    "    Row(struct=Row(name=u'Alice', age=2, gender=u'female')), \n",
    "    Row(struct=Row(name=u'Bob', age=5, gender=u'male'))\n",
    "]\n",
    "```\n",
    "\n",
    "*multiple joins*  \n",
    "You can join multiple dataframes in one chained function.\n",
    "\n",
    "```python\n",
    ">>> df_1.join(df_2, 'name')\\\n",
    "...     .join(df_3, 'gender')\n",
    "...     .collect()\n",
    "[\n",
    "    Row(struct=Row(name=u'Alice', gender=u'female', age=2)), \n",
    "    Row(struct=Row(name=u'Bob', gender=u'male', age=5))\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unions:\n",
    "When you union 2 DFs you need to union them so that the order of the columns match.\n",
    "\n",
    "```python\n",
    ">>> df_1 = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    ">>> df_2 = sc.sql.createDataFrame([(4, 'Eric'), (7, 'Steve')], ['age', 'name'])\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> df = df_1.union(df_2.select(df_1.columns))\n",
    "...          .collect()\n",
    "[\n",
    "    Row(struct=Row(name=u'Alice', age=2)), \n",
    "    Row(struct=Row(name=u'Bob', age=5))\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-structures in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.lit in PySpark:\n",
    "`pyspark.sql.functions.`**`lit`**`(col)`\n",
    "\n",
    "Creates a **Column** of literal value.\n",
    "**Parameters: col** – a literal value.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    ">>> df.withColumn('age', F.lit(100))\\\n",
    "...   .collect()\n",
    "[\n",
    "    Row(struct=Row(age=100, name=u'Alice')), \n",
    "    Row(struct=Row(age=100, name=u'Bob'))\n",
    "]\n",
    ">>> df.withColumn('age', df['age'] > F.lit(3))\\\n",
    "...   .collect()\n",
    "[\n",
    "    Row(struct=Row(age=5, name=u'Bob'))\n",
    "]\n",
    "```\n",
    "Use cases:\n",
    "1. When you are trying to compare cols to literal values.\n",
    "2. When you are trying to assign literal values to columns.\n",
    "\n",
    "\n",
    "**Note**: When you are inside a `F.when(...).otherwise(...)` `F.lit()` is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.col in PySpark:\n",
    "`pyspark.sql.functions.`**`col`**`(col)`\n",
    "\n",
    "Returns a Column based on the given column name.\n",
    "**Parameters: col** – a `column` name.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    ">>> age = df['age']\n",
    ">>> age = F.col('age')\n",
    "```\n",
    "Use cases:\n",
    "1. It returns a column expression that binds to whatever DF it's part of when you run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.struct in PySpark:\n",
    "`pyspark.sql.functions.`**`struct`**`(*cols)`\n",
    "\n",
    "Creates a new struct column.  \n",
    "**Parameters:**\t**cols** – list of column names (string) or list of **Column** expressions.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    "\n",
    ">>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
    "[\n",
    "    Row(struct=Row(age=2, name=u'Alice')), \n",
    "    Row(struct=Row(age=5, name=u'Bob'))\n",
    "]\n",
    "\n",
    ">>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
    "[\n",
    "    Row(struct=Row(age=2, name=u'Alice')), \n",
    "    Row(struct=Row(age=5, name=u'Bob'))\n",
    "]\n",
    "```\n",
    "\n",
    "Use cases:\n",
    "1. When you use `groupBy`, it drop any columns you're not grouping by or aggregating on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### F.coalesce in PySpark:\n",
    "`pyspark.sql.functions.`**`coalesce`**`(*cols)`\n",
    "\n",
    "Returns the first column that is not `null`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> cDf = sc.sql.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    ">>> cDf.show()\n",
    "+----+----+\n",
    "|   a|   b|\n",
    "+----+----+\n",
    "|null|null|\n",
    "|   1|null|\n",
    "|null|   2|\n",
    "+----+----+\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
    "+--------------+\n",
    "|coalesce(a, b)|\n",
    "+--------------+\n",
    "|          null|\n",
    "|             1|\n",
    "|             2|\n",
    "+--------------+\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
    "+----+----+----------------+\n",
    "|   a|   b|coalesce(a, 0.0)|\n",
    "+----+----+----------------+\n",
    "|null|null|             0.0|\n",
    "|   1|null|             1.0|\n",
    "|null|   2|             0.0|\n",
    "+----+----+----------------+\n",
    "```\n",
    "returns the last column that is not `null` and uses a default value is there is none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "```python\n",
    ">>> ....\n",
    ">>> \"comment_present\": F.when(F.col(\"comment_present\").isNull(), False).otherwise(F.col(\"comment_present\")),\n",
    ">>> ...\n",
    "```\n",
    "can be\n",
    "```python\n",
    ">>> ...\n",
    ">>> F.coalesce(F.col(\"comment_present\"), F.lit(False))\n",
    ">>> ...\n",
    "```\n",
    "\n",
    "A use case for `F.coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.explode in PySpark:\n",
    "`pyspark.sql.functions.explode(col)`\n",
    "\n",
    "Returns a new row for each element in the given array or map.\n",
    "\n",
    "**Multi-dimensional arrays collapsing in DataFrames:**  \n",
    "Calling `F.explode(col)` on a 2-D array will flatten all 2-D arrays in `col`.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([(['a'],'a'),\n",
    "...                             (['a', 'b'],'a'),\n",
    "...                             (['c'],'b'),\n",
    "...                             (['d', 'e'],'b')],\n",
    "...                             ['arrays', 'group'])\n",
    ">>> df = df.withColumn('arrays', F.explode('arrays'))\n",
    ">>> df.groupBy('group').agg(F.collect_list('arrays').alias('arrays')).collect()\n",
    "[Row(group=u'b', arrays=[u'c', u'd', u'e']), Row(group=u'a', arrays=[u'a', u'a', u'b'])]\n",
    "```\n",
    "\n",
    "**Creating Multiple rows from an array:**  \n",
    "Calling `F.explode(col)` on an array will create multiple rows for each element in the list in `col`.\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([(1, [\"sample\",\"support\",\"zendesk\"],)], \n",
    "...                             ['col1', 'tags'])\n",
    "+----+--------------------+\n",
    "|col1|                tags|\n",
    "+----+--------------------+\n",
    "|   1|[sample, support,...|\n",
    "+----+--------------------+\n",
    ">>> df.withColumn('tags', F.explode('tags')).show(truncate=False)\n",
    "+----+-------+\n",
    "|col1|tags   |\n",
    "+----+-------+\n",
    "|1   |sample |\n",
    "|1   |support|\n",
    "|1   |zendesk|\n",
    "+----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.json_object() in PySpark:\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('[{\"a\": 1},{\"a\": 2}]',)], ['j'])\n",
    ">>> df.select(F.get_json_object('j', '$')).show()\n",
    "+---------------------+\n",
    "|get_json_object(j, $)|\n",
    "+---------------------+\n",
    "|    [{\"a\":1},{\"a\":2}]|\n",
    "+---------------------+\n",
    "\n",
    ">>> df.select(F.get_json_object('j', '$[0]')).show()\n",
    "+------------------------+\n",
    "|get_json_object(j, $[0])|\n",
    "+------------------------+\n",
    "|                 {\"a\":1}|\n",
    "+------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Operators (F.when & F.otherwise) in PySpark:\n",
    "`pyspark.sql.functions.`**`when`**`(condition, value)`  \n",
    "Evaluates a list of conditions and returns one of multiple possible result expressions. If **Column.otherwise()** is not invoked, None is returned for unmatched conditions.\n",
    "\n",
    "**Parameters:\t\n",
    "condition** – a boolean Column expression.  \n",
    "**value** – a literal value, or a Column expression.\n",
    "\n",
    "`pyspark.sql.functions.`**`otherwise`**`(value)`  \n",
    "Evaluates a list of conditions and returns one of multiple possible result expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions.\n",
    "\n",
    "**Parameters: value** – a literal value, or a **Column** expression.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([('Alice', 2), ('Bob', 5)], ['name','age'])\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
    "+-----+-------------------------------------+\n",
    "| name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
    "+-----+-------------------------------------+\n",
    "|Alice|                                    0|\n",
    "|  Bob|                                    1|\n",
    "+-----+-------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "```python\n",
    ">>> F.when(F.col(\"campaign_medium\").rlike(\"^(cpc|ppc|paidsearch)$\"), F.lit(\"Paid Search\"))\n",
    "... .when(F.col(\"campaign_medium\").rlike(\"^(cpv|cpa|cpp|content-text)$\"), F.lit(\"Other Advertising\"))\n",
    "... .when(F.col(\"campaign_medium\").rlike(\"^(display|cpm|banner)$\"), F.lit(\"Display\"))\n",
    "... .when(F.col(\"campaign_medium\") == F.lit(\"email\"), F.lit(\"Email\"))\n",
    "... .when(F.col(\"campaign_medium\") == F.lit(\"organic\"), F.lit(\"Organic Search\"))\n",
    "... .when(F.col(\"campaign_medium\") == F.lit(\"affiliate\"), F.lit(\"Affiliates\"))\n",
    "... .when(F.col(\"campaign_medium\") == F.lit(\"referral\"), F.lit(\"Referral\"))\n",
    "```\n",
    "\n",
    "You can chain multiple multiple `.when` statements like a switch/case operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Python Functions**\n",
    "* Functions are just variables in python. By writing it like this you save space and complexity as opposed to doing it in-line.\n",
    "\n",
    "```python\n",
    ">>> def derived_session_token_udf():\n",
    ">>>    return F.concat(\n",
    "...        F.col(\"shop_id\").cast(\"string\"), F.lit(\":\"),\n",
    "...        F.col(\"user_token\"), F.lit(\":\"),\n",
    "...        F.col(\"session_token\"), F.lit(\":\"),\n",
    "...        F.year(F.col(timestamp_key)), F.lit(\":\"),\n",
    "...        F.dayofyear(F.col(timestamp_key))\n",
    "...    )\n",
    "\n",
    ">>> new_df = df.withColumn(\"derived_session_token\", derived_session_token_udf())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.to_dicts()` vs `.to_dataframe()`**\n",
    "\n",
    "```python\n",
    ">>> dt = DataTemplate(sc, Contract({'id': {'type': int}, 'foo': {'type': unicode, 'nullable': True}}), \n",
    "                                   {'id': 1})\n",
    ">>> dt.to_dicts([{}])\n",
    "... [{'id': 1}]\n",
    "\n",
    ">>> dt.to_dataframe([{}]).collect()\n",
    "... [Row(foo=None, id=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getItem(key)\n",
    "An expression that gets an item at position ordinal out of a list, or gets an item by key out of a dict.\n",
    "\n",
    "```python\n",
    ">>> df = sc.parallelize([([1, 2], {\"key\": \"value\"})]).toDF([\"l\", \"d\"])\n",
    ">>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
    "+----+------+\n",
    "|l[0]|d[key]|\n",
    "+----+------+\n",
    "|   1| value|\n",
    "+----+------+\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> df.select(df.l[0], df.d[\"key\"]).show()\n",
    "+----+------+\n",
    "|l[0]|d[key]|\n",
    "+----+------+\n",
    "|   1| value|\n",
    "+----+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When multiplying 2 Decimal Types\n",
    "When you multiple 2 decimal types, you can come across values that are `None` or `0`. When the result is `None`, then this usually indicates a **casting** / **overflow** type error. The maximum precision of a **decimal value is 38**.\n",
    "\n",
    "```python\n",
    ">>> df = sc.sql.createDataFrame([(Decimal('113.790000000000000000'), Decimal('1.000000000000000000')),\n",
    "...                              (Decimal('113.790000000000000000'), Decimal('2.000000000000000000')),\n",
    "...                             ['total_price','local_to_usd'])\n",
    "\n",
    ">>> df = df.withColumn('total_price_fx', F.col(\"total_price\") * F.col(\"total_price\"))\\\n",
    "...        .withColumn('total_price_fx2', F.col(\"total_price\") * Decimal('2.50'))\\\n",
    "...        .withColumn('total_price_fx3', F.col(\"total_price\").cast('Decimal(20, 2)') * F.col(\"local_to_usd\"))\n",
    "    \n",
    ">>> df.collect()\n",
    "[Row(total_price=Decimal('113.790000000000000000'), local_to_usd=Decimal('1.000000000000000000'), total_price_fx=None, total_price_fx2=Decimal('284.47500000000000000000'), total_price_fx3=Decimal('113.79000000000000000000')),\n",
    " Row(total_price=Decimal('113.790000000000000000'), local_to_usd=Decimal('2.000000000000000000'), total_price_fx=None, total_price_fx2=Decimal('284.47500000000000000000'), total_price_fx3=Decimal('227.58000000000000000000')),\n",
    " Row(total_price=Decimal('113.790000000000000000'), local_to_usd=Decimal('2.500000000000000000'), total_price_fx=None, total_price_fx2=Decimal('284.47500000000000000000'), total_price_fx3=Decimal('284.47500000000000000000')),\n",
    " Row(total_price=Decimal('113.790000000000006253'), local_to_usd=Decimal('2.500000000000000000'), total_price_fx=None, total_price_fx2=Decimal('284.47500000000001563250'), total_price_fx3=Decimal('284.47500000000000000000'))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `.take(...)` vs `.count()`\n",
    "\n",
    "`.take` could be much slower if it is actually empty.\n",
    "\n",
    "it assumes that it’s easy to satisfy the request, so it only processes a single partition from the final stage, and then gives you the first `(n)` results\n",
    "\n",
    "if it can’t find `(n)` results, it’ll process 2 more partitions\n",
    "\n",
    "and then 4\n",
    "\n",
    "and then 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
