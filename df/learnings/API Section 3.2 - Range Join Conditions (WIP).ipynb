{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"Section 3.2 - Range Join Conditions (WIP)\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipstart</th>\n",
       "      <th>ipend</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ipstart  ipend  loc\n",
       "0        1     10  foo\n",
       "1       11     36  bar\n",
       "2       37     59  baz"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_loc_table = spark.createDataFrame([\n",
    "    (1, 10, \"foo\"), \n",
    "    (11, 36, \"bar\"), \n",
    "    (37, 59, \"baz\"),\n",
    "], [\"ipstart\", \"ipend\", \"loc\"])\n",
    "\n",
    "geo_loc_table.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>inet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  inet\n",
       "0   1    11\n",
       "1   2    38\n",
       "2   3    50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_table = spark.createDataFrame([\n",
    "    (1, 11), \n",
    "    (2, 38), \n",
    "    (3, 50),\n",
    "],[\"id\", \"inet\"])\n",
    "\n",
    "records_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Join Conditions\n",
    "\n",
    "> A naive approach (just specifying this as the range condition) would result in a full cartesian product and a filter that enforces the condition (tested using Spark 2.0). This has a horrible effect on performance, especially if DataFrames are more than a few hundred thousands records.\n",
    "\n",
    "source: http://zachmoshe.com/2016/09/26/efficient-range-joins-with-spark.html\n",
    "\n",
    "> The source of the problem is pretty simple. When you execute join and join condition is not equality based the only thing that Spark can do right now is expand it to Cartesian product followed by filter what is pretty much what happens inside `BroadcastNestedLoopJoin`\n",
    "\n",
    "source: https://stackoverflow.com/questions/37953830/spark-sql-performance-join-on-value-between-min-and-max?answertab=active#tab-top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>inet</th>\n",
       "      <th>ipstart</th>\n",
       "      <th>ipend</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  inet  ipstart  ipend  loc\n",
       "0   1    11       11     36  bar\n",
       "1   2    38       37     59  baz\n",
       "2   3    50       37     59  baz"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_condition = [\n",
    "    records_table['inet'] >= geo_loc_table['ipstart'],\n",
    "    records_table['inet'] <= geo_loc_table['ipend'],\n",
    "]\n",
    "\n",
    "df = records_table.join(geo_loc_table, join_condition, \"left\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "BroadcastNestedLoopJoin BuildRight, LeftOuter, ((inet#252L >= ipstart#245L) && (inet#252L <= ipend#246L))\n",
      ":- Scan ExistingRDD[id#251L,inet#252L]\n",
      "+- BroadcastExchange IdentityBroadcastMode\n",
      "   +- Scan ExistingRDD[ipstart#245L,ipend#246L,loc#247]\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipstart</th>\n",
       "      <th>id</th>\n",
       "      <th>inet</th>\n",
       "      <th>ipend</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>59</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>59</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ipstart  id  inet  ipend  loc\n",
       "0       37   2    38     59  baz\n",
       "1       37   3    50     59  baz\n",
       "2       11   1    11     36  bar"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bisect import bisect_right\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "geo_start_bd = spark.sparkContext.broadcast(map(lambda x: x.ipstart, geo_loc_table\n",
    "    .select(\"ipstart\")\n",
    "    .orderBy(\"ipstart\")\n",
    "    .collect()\n",
    "))\n",
    "\n",
    "def find_le(x):\n",
    "    'Find rightmost value less than or equal to x'\n",
    "    i = bisect_right(geo_start_bd.value, x)\n",
    "    if i:\n",
    "        return geo_start_bd.value[i-1]\n",
    "    return None\n",
    "\n",
    "records_table_with_ipstart = records_table.withColumn(\n",
    "    \"ipstart\", udf(find_le, LongType())(\"inet\")\n",
    ")\n",
    "\n",
    "df = records_table_with_ipstart.join(geo_loc_table, [\"ipstart\"], \"left\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [ipstart#272L, id#251L, inet#252L, ipend#246L, loc#247]\n",
      "+- SortMergeJoin [ipstart#272L], [ipstart#245L], LeftOuter\n",
      "   :- *(2) Sort [ipstart#272L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(ipstart#272L, 200)\n",
      "   :     +- *(1) Project [id#251L, inet#252L, pythonUDF0#281L AS ipstart#272L]\n",
      "   :        +- BatchEvalPython [find_le(inet#252L)], [id#251L, inet#252L, pythonUDF0#281L]\n",
      "   :           +- Scan ExistingRDD[id#251L,inet#252L]\n",
      "   +- *(3) Sort [ipstart#245L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(ipstart#245L, 200)\n",
      "         +- Scan ExistingRDD[ipstart#245L,ipend#246L,loc#247]\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
