{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CompanyBroadcast\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = spark.read.option(\"header\", True).csv(\"data/company_data/companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+\n",
      "| id|      company|country_id|\n",
      "+---+-------------+----------+\n",
      "|  1|       Mybuzz|        11|\n",
      "|  2|Chatterbridge|         3|\n",
      "|  3|       Skyble|         7|\n",
      "|  4|   Brainverse|         4|\n",
      "|  5|   Jabbertype|         7|\n",
      "|  6|     Zoombeat|        12|\n",
      "|  7|     Tanoodle|         8|\n",
      "|  8|      Feedmix|        13|\n",
      "|  9|      Meembee|        20|\n",
      "| 10|     Riffpath|         7|\n",
      "| 11|      Dynabox|        19|\n",
      "| 12|   Browsetype|         3|\n",
      "| 13|      Dynazzy|        20|\n",
      "| 14|       Demizz|        19|\n",
      "| 15|    Riffpedia|        18|\n",
      "| 16|         Zava|        13|\n",
      "| 17|      Pixonyx|        20|\n",
      "| 18|       Yambee|        15|\n",
      "| 19|        Yombu|         7|\n",
      "| 20|        Voomm|        14|\n",
      "+---+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "company_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = spark.read.option(\"header\", True).csv(\"data/company_data/countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|      country|\n",
      "+---+-------------+\n",
      "|  1|       Russia|\n",
      "|  2|        Yemen|\n",
      "|  3|       Sweden|\n",
      "|  4|  Philippines|\n",
      "|  5|     Malaysia|\n",
      "|  6|       France|\n",
      "|  7|       Greece|\n",
      "|  8|    Argentina|\n",
      "|  9|      Ecuador|\n",
      "| 10|         Peru|\n",
      "| 11|        China|\n",
      "| 12|United States|\n",
      "| 13|        Malta|\n",
      "| 14|      Somalia|\n",
      "| 15|      Nigeria|\n",
      "| 16|        Italy|\n",
      "| 17|        Spain|\n",
      "| 18|        Niger|\n",
      "| 19|   Bangladesh|\n",
      "| 20|      Ukraine|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "join_df = company_df.join(broadcast(country_df), company_df[\"country_id\"] == country_df[\"id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+---+-------------+\n",
      "| id|      company|country_id| id|      country|\n",
      "+---+-------------+----------+---+-------------+\n",
      "|  1|       Mybuzz|        11| 11|        China|\n",
      "|  2|Chatterbridge|         3|  3|       Sweden|\n",
      "|  3|       Skyble|         7|  7|       Greece|\n",
      "|  4|   Brainverse|         4|  4|  Philippines|\n",
      "|  5|   Jabbertype|         7|  7|       Greece|\n",
      "|  6|     Zoombeat|        12| 12|United States|\n",
      "|  7|     Tanoodle|         8|  8|    Argentina|\n",
      "|  8|      Feedmix|        13| 13|        Malta|\n",
      "|  9|      Meembee|        20| 20|      Ukraine|\n",
      "| 10|     Riffpath|         7|  7|       Greece|\n",
      "| 11|      Dynabox|        19| 19|   Bangladesh|\n",
      "| 12|   Browsetype|         3|  3|       Sweden|\n",
      "| 13|      Dynazzy|        20| 20|      Ukraine|\n",
      "| 14|       Demizz|        19| 19|   Bangladesh|\n",
      "| 15|    Riffpedia|        18| 18|        Niger|\n",
      "| 16|         Zava|        13| 13|        Malta|\n",
      "| 17|      Pixonyx|        20| 20|      Ukraine|\n",
      "| 18|       Yambee|        15| 15|      Nigeria|\n",
      "| 19|        Yombu|         7|  7|       Greece|\n",
      "| 20|        Voomm|        14| 14|      Somalia|\n",
      "+---+-------------+----------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [country_id#19], [id#57], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(country_id#19)\n",
      "   :  +- FileScan csv [id#17,company#18,country_id#19] Batched: false, DataFilters: [isnotnull(country_id#19)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/d:/BigData/Spark/data/company_data/companies.csv], PartitionFilters: [], PushedFilters: [IsNotNull(country_id)], ReadSchema: struct<id:string,company:string,country_id:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=134]\n",
      "      +- Filter isnotnull(id#57)\n",
      "         +- FileScan csv [id#57,country#58] Batched: false, DataFilters: [isnotnull(id#57)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/d:/BigData/Spark/data/company_data/countries.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|   company|      country|\n",
      "+----------+-------------+\n",
      "|  Zoombeat|United States|\n",
      "|   Skilith|United States|\n",
      "|    Meedoo|United States|\n",
      "|     Twimm|United States|\n",
      "|Bubbletube|United States|\n",
      "|   Innojam|United States|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.select(\"company\", \"country\").filter(join_df[\"country\"] == \"United States\") .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
