{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fd7f630",
   "metadata": {},
   "source": [
    "# Spark DF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73855f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_311'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('JAVA_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96eacb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbdeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\",\n",
    "                                    \"temp\").appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb9a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df = spark.read.format(\"csv\") \\\n",
    "               .option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .load(\"data/financial/account.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865da2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------------+------+\n",
      "|account_id|district_id|       frequency|  date|\n",
      "+----------+-----------+----------------+------+\n",
      "|       576|         55|POPLATEK MESICNE|930101|\n",
      "|      3818|         74|POPLATEK MESICNE|930101|\n",
      "|       704|         55|POPLATEK MESICNE|930101|\n",
      "|      2378|         16|POPLATEK MESICNE|930101|\n",
      "|      2632|         24|POPLATEK MESICNE|930102|\n",
      "+----------+-----------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c50c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = spark.read \\\n",
    "             .option(\"header\", \"true\") \\\n",
    "             .option(\"delimiter\", \";\") \\\n",
    "             .csv(\"data/financial/trans.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35984766",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+\n",
      "|trans_id|account_id|  date|  type|operation| amount|balance|k_symbol|bank|account|\n",
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+\n",
      "|  695247|      2378|930101|PRIJEM|    VKLAD| 700.00| 700.00|    null|null|   null|\n",
      "|  171812|       576|930101|PRIJEM|    VKLAD| 900.00| 900.00|    null|null|   null|\n",
      "|  207264|       704|930101|PRIJEM|    VKLAD|1000.00|1000.00|    null|null|   null|\n",
      "| 1117247|      3818|930101|PRIJEM|    VKLAD| 600.00| 600.00|    null|null|   null|\n",
      "|  579373|      1972|930102|PRIJEM|    VKLAD| 400.00| 400.00|    null|null|   null|\n",
      "+--------+----------+------+------+---------+-------+-------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec12142",
   "metadata": {},
   "source": [
    "# Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cacbd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------+\n",
      "|       frequency|district_id|acc_no|\n",
      "+----------------+-----------+------+\n",
      "|POPLATEK MESICNE|         55|   576|\n",
      "|POPLATEK MESICNE|         74|  3818|\n",
      "+----------------+-----------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "account_df.select(\n",
    "    account_df.frequency,\n",
    "    account_df[\"district_id\"],\n",
    "    col(\"account_id\").alias(\"acc_no\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c629a",
   "metadata": {},
   "source": [
    "# Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d8585de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+--------+---------+--------+----+-------+\n",
      "|trans_id|account_id|  date|  type|operation|  amount|  balance|k_symbol|bank|account|\n",
      "+--------+----------+------+------+---------+--------+---------+--------+----+-------+\n",
      "|  980497|      3345|980520| VYDAJ|    VYBER| 1360.00|    -1.60|    null|null|   null|\n",
      "| 2777508|      9192|981024| VYDAJ|    VYBER|22700.00|  -100.60|    null|null|   null|\n",
      "|  417609|      1416|950531| VYDAJ|    VYBER|   30.00| -1000.10|  SLUZBY|null|   null|\n",
      "|  684290|      2335|980831| VYDAJ|    VYBER|   14.60| -1000.60|  SLUZBY|null|   null|\n",
      "| 1599380|      5429|980608|PRIJEM|    VKLAD|13318.00|-10010.40|    null|null|   null|\n",
      "+--------+----------+------+------+---------+--------+---------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.sort(trans_df.balance.asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40916bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+--------+--------+--------+----+-------+\n",
      "|trans_id|account_id|  date|  type|operation|  amount| balance|k_symbol|bank|account|\n",
      "+--------+----------+------+------+---------+--------+--------+--------+----+-------+\n",
      "|  289894|       993|980607|PRIJEM|    VKLAD|28918.00|99999.60|    null|null|   null|\n",
      "|  529703|      1805|980805| VYDAJ|    VYBER| 9500.00|99999.40|    null|null|   null|\n",
      "|  810680|      2762|940427| VYDAJ|    VYBER|29100.00|99998.80|    null|null|   null|\n",
      "| 3459917|       200|980331|PRIJEM|     null|  413.30|99998.60|    UROK|null|   null|\n",
      "| 3459920|       200|980630|PRIJEM|     null|  428.50|99998.40|    UROK|null|   null|\n",
      "+--------+----------+------+------+---------+--------+--------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.sort(trans_df.balance.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6737b8",
   "metadata": {},
   "source": [
    "# When-Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5a7cf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|trans_id|symbol|\n",
      "+--------+------+\n",
      "|  695247|     O|\n",
      "|  171812|     O|\n",
      "|  207264|     O|\n",
      "| 1117247|     O|\n",
      "|  579373|     O|\n",
      "|  771035|     O|\n",
      "|  452728|     O|\n",
      "|  725751|     O|\n",
      "|  497211|     O|\n",
      "|  232960|     O|\n",
      "+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "trans_df.select(\n",
    "    trans_df.trans_id,\n",
    "    when(trans_df.k_symbol==\"SLUZBY\", \"S\") \\\n",
    "        .when(trans_df.k_symbol==None ,\"A\") \\\n",
    "        .otherwise(\"O\").alias(\"symbol\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffdcef6",
   "metadata": {},
   "source": [
    "# Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac2d44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+--------------+-------+--------+--------+----+--------+\n",
      "|trans_id|account_id|  date|  type|     operation| amount| balance|k_symbol|bank| account|\n",
      "+--------+----------+------+------+--------------+-------+--------+--------+----+--------+\n",
      "|  637742|      2177|930105|PRIJEM| PREVOD Z UCTU|5123.00| 5923.00|  DUCHOD|  YZ|62457513|\n",
      "|  579374|      1972|930107|PRIJEM| PREVOD Z UCTU|5298.00| 5698.00|  DUCHOD|  UV|14132887|\n",
      "| 1049882|      3592|930110|PRIJEM| PREVOD Z UCTU|6007.00| 6607.00|  DUCHOD|  MN|73166322|\n",
      "|  171813|       576|930111|PRIJEM| PREVOD Z UCTU|6207.00| 7107.00|  DUCHOD|  YZ|30300313|\n",
      "|  689828|      2357|930112|PRIJEM| PREVOD Z UCTU|6434.00| 7234.00|  DUCHOD|  OP|34144538|\n",
      "|  477639|      1628|930112|PRIJEM| PREVOD Z UCTU|4276.00| 4976.00|  DUCHOD|  UV|15916598|\n",
      "|  439036|      1493|930113|PRIJEM| PREVOD Z UCTU|5009.00| 5209.00|  DUCHOD|  AB|54522466|\n",
      "|  480215|      1637|930113|PRIJEM| PREVOD Z UCTU|5718.00| 6218.00|  DUCHOD|  UV|45134812|\n",
      "| 2908689|      9635|930114|PRIJEM| PREVOD Z UCTU|4470.00| 4870.00|  DUCHOD|  CD|37906074|\n",
      "|  637743|      2177|930205|PRIJEM| PREVOD Z UCTU|5123.00|11067.50|  DUCHOD|  YZ|62457513|\n",
      "|  498391|      1699|930205|PRIJEM| PREVOD Z UCTU|5224.00| 6024.00|  DUCHOD|  CD|94008880|\n",
      "|  529774|      1806|930205|PRIJEM| PREVOD Z UCTU|5029.00| 5429.00|  DUCHOD|  UV|39912659|\n",
      "|  224344|       764|930205|PRIJEM| PREVOD Z UCTU|6238.00| 7338.00|  DUCHOD|  UV|30560720|\n",
      "|   57435|       192|930206|PRIJEM| PREVOD Z UCTU|5859.00| 6159.00|  DUCHOD|  GH|15384018|\n",
      "| 1132664|      3871|930206|PRIJEM| PREVOD Z UCTU|4356.00| 4656.00|  DUCHOD|  GH|31564527|\n",
      "|  506452|      1730|930206|PRIJEM| PREVOD Z UCTU|6689.00| 7589.00|  DUCHOD|  ST|43294817|\n",
      "|  695340|      2378|930207| VYDAJ|PREVOD NA UCET|9612.00|80033.70|    SIPO|  EF| 1222903|\n",
      "|  129504|       435|930207|PRIJEM| PREVOD Z UCTU|5039.00| 5439.00|  DUCHOD|  MN|22131634|\n",
      "|  347545|      1179|930207|PRIJEM| PREVOD Z UCTU|4212.00| 4912.00|  DUCHOD|  WX|72322171|\n",
      "|  579375|      1972|930207|PRIJEM| PREVOD Z UCTU|5298.00|11015.10|  DUCHOD|  UV|14132887|\n",
      "+--------+----------+------+------+--------------+-------+--------+--------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "trans_df.filter(expr(\"account is not null and k_symbol is not null\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5ca9a",
   "metadata": {},
   "source": [
    "# Lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aec5f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|account_id|lit_constant|\n",
      "+----------+------------+\n",
      "|       576|          AB|\n",
      "|      3818|          AB|\n",
      "|       704|          AB|\n",
      "|      2378|          AB|\n",
      "|      2632|          AB|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "account_df.select(\n",
    "    account_df.account_id,\n",
    "    lit(\"AB\").alias(\"lit_constant\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6715ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = [\n",
    "    (\"James\", [[\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"]]),\n",
    "    (\"Michael\", [[\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"]]),\n",
    "    (\"Robert\", [[\"CSharp\", \"VB\"], [\"Spark\", \"Python\"]])\n",
    "]\n",
    "df = spark.createDataFrame(data=arr_data, schema=['name', 'subjects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d092da07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a3974",
   "metadata": {},
   "source": [
    "# Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "820c524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|name   |col               |\n",
      "+-------+------------------+\n",
      "|James  |[Java, Scala, C++]|\n",
      "|James  |[Spark, Java]     |\n",
      "|Michael|[Spark, Java, C++]|\n",
      "|Michael|[Spark, Java]     |\n",
      "|Robert |[CSharp, VB]      |\n",
      "|Robert |[Spark, Python]   |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df.name, explode(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e31c4",
   "metadata": {},
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb53b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|name   |flatten(subjects)              |\n",
      "+-------+-------------------------------+\n",
      "|James  |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "df.select(df.name, flatten(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "126aa511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"James\", \"Sales\", 3000), \n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100), \n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000), \n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900), \n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000), \n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "]\n",
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d723cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c4bea",
   "metadata": {},
   "source": [
    "# Window Function - row_number\n",
    "row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69e8a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fb119",
   "metadata": {},
   "source": [
    "# Window Function - rank\n",
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a570839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971447f",
   "metadata": {},
   "source": [
    "# Window Function - dense_rank\n",
    "* dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps.\n",
    "* This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "607da20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da040e",
   "metadata": {},
   "source": [
    "# Window Function - lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d32160cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lag\", lag(\"salary\", 2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e7005",
   "metadata": {},
   "source": [
    "# Window Function - lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "020d0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb676f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
