{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73855f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_311'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('JAVA_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96eacb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbdeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\",\n",
    "                                    \"temp\").appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"US\", 10), (\"UK\", 20), (\"India\", 30), (\"China\", 40), (\"Japan\", 50)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68e5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-----+---+\n",
      "|_1   |_2 |\n",
      "+-----+---+\n",
      "|US   |10 |\n",
      "|UK   |20 |\n",
      "|India|30 |\n",
      "|China|40 |\n",
      "|Japan|50 |\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621572e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|country|id |\n",
      "+-------+---+\n",
      "|US     |10 |\n",
      "|UK     |20 |\n",
      "|India  |30 |\n",
      "|China  |40 |\n",
      "|Japan  |50 |\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"country\", \"id\"]\n",
    "df = rdd.toDF(cols)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa129543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|country|id |\n",
      "+-------+---+\n",
      "|US     |10 |\n",
      "|UK     |20 |\n",
      "|India  |30 |\n",
      "|China  |40 |\n",
      "|Japan  |50 |\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"country\", \"id\"]\n",
    "df = spark.createDataFrame(rdd, schema=deptColumns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557a4c90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: string (nullable = false)\n",
      "\n",
      "Exception caught: An error occurred while calling o346.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 1 times, most recent failure: Lost task 2.0 in stage 39.0 (TID 104) (DESKTOP-SSC2TF1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\session.py\", line 1292, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 2000, in verify\n",
      "    verify_value(obj)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1978, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1999, in verify\n",
      "    if not verify_nullability(obj):\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1856, in verify_nullability\n",
      "    raise ValueError(new_msg(\"This field is not nullable, but got None\"))\n",
      "ValueError: field id: This field is not nullable, but got None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\session.py\", line 1292, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 2000, in verify\n",
      "    verify_value(obj)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1978, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1999, in verify\n",
      "    if not verify_nullability(obj):\n",
      "  File \"D:\\Apache\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\types.py\", line 1856, in verify_nullability\n",
      "    raise ValueError(new_msg(\"This field is not nullable, but got None\"))\n",
      "ValueError: field id: This field is not nullable, but got None\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "data = [(\"US\", 10), (\"UK\", 20), (\"India\", 30), (\"China\", 40), (\"Japan\", None)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "colSchema = StructType([\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('id', StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=colSchema)\n",
    "\n",
    "df.printSchema()\n",
    "try:\n",
    "    df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Exception caused during action: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fa20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
