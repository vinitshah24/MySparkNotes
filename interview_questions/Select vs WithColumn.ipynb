{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6948be5-3937-450d-bda7-f0ed21617ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050aba7b-ec12-483c-b857-8e80d0e7d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75148f9b-460c-488a-b125-087bdeeb9f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-------+\n",
      "| id|        name|salary|company|\n",
      "+---+------------+------+-------+\n",
      "|  1|  Adam Smith|900000|    ABC|\n",
      "|  2| Brian Jones|800000|    XYZ|\n",
      "|  3| Chris Milne|580000|    ABC|\n",
      "|  4|David Harris| 80000|    PQE|\n",
      "+---+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [\n",
    "    (1, \"Adam Smith\", 900000, \"ABC\"),\n",
    "    (2, \"Brian Jones\", 800000, \"XYZ\"),\n",
    "    (3, \"Chris Milne\", 580000, \"ABC\"),\n",
    "    (4, \"David Harris\", 80000, \"PQE\"),\n",
    "]\n",
    "df = spark.createDataFrame(data_list, [\"id\", \"name\", \"salary\", \"company\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea643d9-f718-4912-94ec-e24482c4197e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-------+---------+----------+\n",
      "| id|        name|salary|company|increment|new_salary|\n",
      "+---+------------+------+-------+---------+----------+\n",
      "|  1|  Adam Smith|900000|    ABC|  90000.0|  990000.0|\n",
      "|  2| Brian Jones|800000|    XYZ|  80000.0|  880000.0|\n",
      "|  3| Chris Milne|580000|    ABC|  58000.0|  638000.0|\n",
      "|  4|David Harris| 80000|    PQE|   8000.0|   88000.0|\n",
      "+---+------------+------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project [id#0L, name#1, salary#2L, company#3, increment#25, ('salary + 'increment) AS new_salary#31]\n",
      "+- Project [id#0L, name#1, salary#2L, company#3, (cast((salary#2L * cast(10 as bigint)) as double) / cast(100 as double)) AS increment#25]\n",
      "   +- LogicalRDD [id#0L, name#1, salary#2L, company#3], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string, salary: bigint, company: string, increment: double, new_salary: double\n",
      "Project [id#0L, name#1, salary#2L, company#3, increment#25, (cast(salary#2L as double) + increment#25) AS new_salary#31]\n",
      "+- Project [id#0L, name#1, salary#2L, company#3, (cast((salary#2L * cast(10 as bigint)) as double) / cast(100 as double)) AS increment#25]\n",
      "   +- LogicalRDD [id#0L, name#1, salary#2L, company#3], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#0L, name#1, salary#2L, company#3, increment#25, (cast(salary#2L as double) + increment#25) AS new_salary#31]\n",
      "+- Project [id#0L, name#1, salary#2L, company#3, (cast((salary#2L * 10) as double) / 100.0) AS increment#25]\n",
      "   +- LogicalRDD [id#0L, name#1, salary#2L, company#3], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#0L, name#1, salary#2L, company#3, increment#25, (cast(salary#2L as double) + increment#25) AS new_salary#31]\n",
      "+- *(1) Project [id#0L, name#1, salary#2L, company#3, (cast((salary#2L * 10) as double) / 100.0) AS increment#25]\n",
      "   +- *(1) Scan ExistingRDD[id#0L,name#1,salary#2L,company#3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn(\"increment\", expr(\"salary * 10 / 100\")).withColumn(\n",
    "    \"new_salary\", expr(\"salary + increment\")\n",
    ")\n",
    "new_df.show()\n",
    "new_df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1faa6c0-ff3f-4836-a3f5-c646d669e918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+---------+----------+\n",
      "| id|        name|salary|increment|new_salary|\n",
      "+---+------------+------+---------+----------+\n",
      "|  1|  Adam Smith|900000|  90000.0|  990000.0|\n",
      "|  2| Brian Jones|800000|  80000.0|  880000.0|\n",
      "|  3| Chris Milne|580000|  58000.0|  638000.0|\n",
      "|  4|David Harris| 80000|   8000.0|   88000.0|\n",
      "+---+------------+------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"salary\",\n",
    "    expr(\"salary * 10 / 100\").alias(\"increment\"),\n",
    "    expr(\"salary + increment\").alias(\"new_salary\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd365da8-70c9-438b-8802-b061cfdb3e8d",
   "metadata": {},
   "source": [
    "## Performance Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56744991-b395-45f4-9566-765ff041c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+----+----+\n",
      "| id|name|foo1|foo2|foo3|foo4|foo5|\n",
      "+---+----+----+----+----+----+----+\n",
      "|  1| abc|null|null|null|null|null|\n",
      "|  2| xyz|null|null|null|null|null|\n",
      "+---+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(id=1, name=\"abc\"), Row(id=2, name=\"xyz\")])\n",
    "dummy_col_list = [\"foo1\", \"foo2\", \"foo3\", \"foo4\", \"foo5\"]\n",
    "for col_name in dummy_col_list:\n",
    "    df = df.withColumn(col_name, lit(None).cast(\"string\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6900fe84-ea73-42bf-8882-069a097f47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [id#91L, name#92, foo1#95, foo2#99, foo3#104, foo4#110, cast(null as string) AS foo5#117]\n",
      "+- Project [id#91L, name#92, foo1#95, foo2#99, foo3#104, cast(null as string) AS foo4#110]\n",
      "   +- Project [id#91L, name#92, foo1#95, foo2#99, cast(null as string) AS foo3#104]\n",
      "      +- Project [id#91L, name#92, foo1#95, cast(null as string) AS foo2#99]\n",
      "         +- Project [id#91L, name#92, cast(null as string) AS foo1#95]\n",
      "            +- LogicalRDD [id#91L, name#92], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string, foo1: string, foo2: string, foo3: string, foo4: string, foo5: string\n",
      "Project [id#91L, name#92, foo1#95, foo2#99, foo3#104, foo4#110, cast(null as string) AS foo5#117]\n",
      "+- Project [id#91L, name#92, foo1#95, foo2#99, foo3#104, cast(null as string) AS foo4#110]\n",
      "   +- Project [id#91L, name#92, foo1#95, foo2#99, cast(null as string) AS foo3#104]\n",
      "      +- Project [id#91L, name#92, foo1#95, cast(null as string) AS foo2#99]\n",
      "         +- Project [id#91L, name#92, cast(null as string) AS foo1#95]\n",
      "            +- LogicalRDD [id#91L, name#92], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#91L, name#92, null AS foo1#95, null AS foo2#99, null AS foo3#104, null AS foo4#110, null AS foo5#117]\n",
      "+- LogicalRDD [id#91L, name#92], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#91L, name#92, null AS foo1#95, null AS foo2#99, null AS foo3#104, null AS foo4#110, null AS foo5#117]\n",
      "+- *(1) Scan ExistingRDD[id#91L,name#92]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a1ecb-69a4-4e10-8ded-1e5ca7048859",
   "metadata": {},
   "source": [
    "Okay. There are multiple Project nodes, so what? It's not in the Physical Plan, and Spark will finally execute the selected Physical Plan. So, there shouldn't be any performance regression because of this during execution.\n",
    "\n",
    "Well, the performance degradation happens before it even reaches the Physical Plan.\n",
    "\n",
    "### Cause of Performance Degradation\n",
    "Each time withColumn is used to add a column in the dataframe, Spark’s Catalyst optimizer re-evaluates the whole plan repeatedly. This adds up fast and strains performance.\n",
    "\n",
    "**The surprising part? You might not notice it until you dig into Spark’s Logical Plans.**\n",
    "\n",
    "This issue is not so obvious because it doesn't show up in the SparkUI. Your job that might take only 5mins to complete, can end up taking 5x more time because of multiple withColumn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4846d-5bb5-41b0-900b-5caa26e9f8ce",
   "metadata": {},
   "source": [
    "### Solution #1: Using .withColumns() for Spark >= 3.3\n",
    "Starting from Spark 3.3, withColumns() transformation is available to use, that takes a dictionary of string and Column datatype.\n",
    "\n",
    "Only a single Project node present in the Logical Plan.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef74a3fc-553d-4830-a7fa-959324684b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [id#154L, name#155, cast(null as string) AS foo1#158, cast(null as string) AS foo2#159, cast(null as string) AS foo3#160, cast(null as string) AS foo4#161, cast(null as string) AS foo5#162]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string, foo1: string, foo2: string, foo3: string, foo4: string, foo5: string\n",
      "Project [id#154L, name#155, cast(null as string) AS foo1#158, cast(null as string) AS foo2#159, cast(null as string) AS foo3#160, cast(null as string) AS foo4#161, cast(null as string) AS foo5#162]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#154L, name#155, null AS foo1#158, null AS foo2#159, null AS foo3#160, null AS foo4#161, null AS foo5#162]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#154L, name#155, null AS foo1#158, null AS foo2#159, null AS foo3#160, null AS foo4#161, null AS foo5#162]\n",
      "+- *(1) Scan ExistingRDD[id#154L,name#155]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([Row(id=1, name=\"abc\"), Row(id=2, name=\"xyz\")])\n",
    "dummy_col_val_map = {\n",
    "    \"foo1\": lit(None).cast(\"string\"),\n",
    "    \"foo2\": lit(None).cast(\"string\"),\n",
    "    \"foo3\": lit(None).cast(\"string\"),\n",
    "    \"foo4\": lit(None).cast(\"string\"),\n",
    "    \"foo5\": lit(None).cast(\"string\"),\n",
    "}\n",
    "\n",
    "# Adding columns using withColumns\n",
    "df1 = df.withColumns(dummy_col_val_map)\n",
    "\n",
    "# Checking both Analytical and Physical Plan\n",
    "df1.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84805e43-465b-4808-9290-90128ac954c3",
   "metadata": {},
   "source": [
    "### Solution #2: Using .select() with an alias\n",
    "Another way to achieve the same is via `.select` with `alias` or `.selectExpr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be93ecf-5f1b-46e5-aa1b-dc602a7207b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, cast(null as string) AS foo1#170, cast(null as string) AS foo2#171, cast(null as string) AS foo3#172, cast(null as string) AS foo4#173, cast(null as string) AS foo5#174]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string, foo1: string, foo2: string, foo3: string, foo4: string, foo5: string\n",
      "Project [id#154L, name#155, cast(null as string) AS foo1#170, cast(null as string) AS foo2#171, cast(null as string) AS foo3#172, cast(null as string) AS foo4#173, cast(null as string) AS foo5#174]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#154L, name#155, null AS foo1#170, null AS foo2#171, null AS foo3#172, null AS foo4#173, null AS foo5#174]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#154L, name#155, null AS foo1#170, null AS foo2#171, null AS foo3#172, null AS foo4#173, null AS foo5#174]\n",
      "+- *(1) Scan ExistingRDD[id#154L,name#155]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(\n",
    "    \"*\", *[cvalue.alias(cname) for cname, cvalue in dummy_col_val_map.items()]\n",
    ")\n",
    "\n",
    "df2.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d23fa-b1d4-42a2-b119-adf7551f59c8",
   "metadata": {},
   "source": [
    "### FAQs\n",
    "Every time I explain this, there are some follow up questions that engineers ask:\n",
    "\n",
    "**Is this the only case when `.withColumn` is used in for loop?**\n",
    "\n",
    "No. The same issue happens when we use multiple `.withColumn` outside loop also. We can look into the Logical Plan again to verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c0b1c1-62a3-4f7f-be86-1d40ddf595a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [id#154L, name#155, foo1#182, foo2#186, foo3#191, foo4#197, cast(null as string) AS foo5#204]\n",
      "+- Project [id#154L, name#155, foo1#182, foo2#186, foo3#191, cast(null as string) AS foo4#197]\n",
      "   +- Project [id#154L, name#155, foo1#182, foo2#186, cast(null as string) AS foo3#191]\n",
      "      +- Project [id#154L, name#155, foo1#182, cast(null as string) AS foo2#186]\n",
      "         +- Project [id#154L, name#155, cast(null as string) AS foo1#182]\n",
      "            +- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string, foo1: string, foo2: string, foo3: string, foo4: string, foo5: string\n",
      "Project [id#154L, name#155, foo1#182, foo2#186, foo3#191, foo4#197, cast(null as string) AS foo5#204]\n",
      "+- Project [id#154L, name#155, foo1#182, foo2#186, foo3#191, cast(null as string) AS foo4#197]\n",
      "   +- Project [id#154L, name#155, foo1#182, foo2#186, cast(null as string) AS foo3#191]\n",
      "      +- Project [id#154L, name#155, foo1#182, cast(null as string) AS foo2#186]\n",
      "         +- Project [id#154L, name#155, cast(null as string) AS foo1#182]\n",
      "            +- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#154L, name#155, null AS foo1#182, null AS foo2#186, null AS foo3#191, null AS foo4#197, null AS foo5#204]\n",
      "+- LogicalRDD [id#154L, name#155], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#154L, name#155, null AS foo1#182, null AS foo2#186, null AS foo3#191, null AS foo4#197, null AS foo5#204]\n",
      "+- *(1) Scan ExistingRDD[id#154L,name#155]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = (\n",
    "    df.withColumn(\"foo1\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"foo2\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"foo3\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"foo4\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"foo5\", lit(None).cast(\"string\"))\n",
    ")\n",
    "\n",
    "df4.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db266ba6-f6eb-4d59-b72c-6608b0fcf132",
   "metadata": {},
   "source": [
    "**Should we not use .withColumn at all then?**\n",
    "\n",
    "If the number of columns being added are fairly low, we can use it, it wouldn't make much of a difference.\n",
    "But if you are planning to write a code, that you think can further be extended based on the upcoming requirements, I would recommend using .withColumns or other 2 options.\n",
    "\n",
    "**How many .withColumn are too many that can cause degradation?**\n",
    "\n",
    "There are no specific numbers of columns, but if you have like 100s of withColumn with some transformation logics, chances are your Spark Job can do so much better.\n",
    "\n",
    "**How can we look into SparkUI then if this is the issue?**\n",
    "\n",
    "The issue won't be so easily visible on SparkUI, the starting point is to compare the Job Uptime and the time taken by the Jobs in Spark UI Jobs tab.\n",
    "\n",
    "If all the jobs are finishing quickly but total Uptime is significantly higher, chances are multiple withColumn are the potential cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058b7e5-fcfa-4f9e-8ce5-4685312f39ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
