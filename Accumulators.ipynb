{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73855f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_311'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('JAVA_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96eacb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbdeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25b345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001421837F730>\n",
      "<SparkContext master=local[4] appName=SparkSQL>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"SparkSQL\").getOrCreate()\n",
    "print(spark)\n",
    "print(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07b452",
   "metadata": {},
   "source": [
    "#### Same Spark session is used. Only one SparkSession is created when used getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e80dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001421837F730>\n",
      "<SparkContext master=local[4] appName=SparkSQL>\n"
     ]
    }
   ],
   "source": [
    "spark1 = SparkSession.builder.master(\"local[*]\").appName(\"SparkSQL\").getOrCreate()\n",
    "print(spark1)\n",
    "print(spark1.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89c400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-SSC2TF1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1421837f730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756c6110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa4ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    (\"India\", \"USA\", '5'), \n",
    "    (\"India\", \"China\", '7'),\n",
    "    (\"UK\", \"India\", 'three'), \n",
    "    (\"China\", \"Africa\", '6'),\n",
    "    (\"Japan\", \"UK\", 'Five')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0eb9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data_list).toDF(\"Source\", \"Destination\", \"Shipments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90d0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n",
      "|Source|Destination|Shipments|\n",
      "+------+-----------+---------+\n",
      "| India|        USA|        5|\n",
      "| India|      China|        7|\n",
      "|    UK|      India|    three|\n",
      "| China|     Africa|        6|\n",
      "| Japan|         UK|     Five|\n",
      "+------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120a560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_record_acc = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f54f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_records(shipments: str) -> int:\n",
    "    data = None\n",
    "    try:\n",
    "        data = int(shipments)\n",
    "    except ValueError:\n",
    "        bad_record_acc.add(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb48a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.clean_records(shipments: str) -> int>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"clean_records_udf\", clean_records, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad6f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+--------------+\n",
      "|Source|Destination|Shipments|ShipmentsCount|\n",
      "+------+-----------+---------+--------------+\n",
      "| India|        USA|        5|             5|\n",
      "| India|      China|        7|             7|\n",
      "|    UK|      India|    three|          null|\n",
      "| China|     Africa|        6|             6|\n",
      "| Japan|         UK|     Five|          null|\n",
      "+------+-----------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ShipmentsCount\", expr(\"clean_records_udf(shipments)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96b4f81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Record Count: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bad Record Count: {bad_record_acc.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1e422",
   "metadata": {},
   "source": [
    "# Record Count Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ca40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_acc = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec8a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f319612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Source='India', Destination='USA', Shipments='5'),\n",
       " Row(Source='India', Destination='China', Shipments='7'),\n",
       " Row(Source='UK', Destination='India', Shipments='three'),\n",
       " Row(Source='China', Destination='Africa', Shipments='6'),\n",
       " Row(Source='Japan', Destination='UK', Shipments='Five')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02cdad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: records_acc.add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a547a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record Count: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Record Count: {records_acc.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3d7c4",
   "metadata": {},
   "source": [
    "# Total Shipments Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf3e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipments_acc = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcce97c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()[0].Shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1057b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(row):\n",
    "    shipment = row.Shipments\n",
    "    try:\n",
    "        if shipment:\n",
    "            shipments_acc.add(int(shipment))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c4494a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: get_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d45cb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Shipments Count: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Shipments Count: {shipments_acc.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c9263",
   "metadata": {},
   "source": [
    "## When to use accumulators?\n",
    "    INSIDE ACTION FUNCTIONS\n",
    "* We have used accumulators inside the foreach(). \n",
    "* foreach() is an action and action functions are the right location to use accumulators. \n",
    "* foreach() are when to update the state of external variables and systems aka. it causes side effects and foreach() is an appropriate place to update accumulators and more over foreach() is an action function and not a transformation function and hence it is the correct place to manipulate accumulators.\n",
    "\n",
    "## When NOT to use accumulators?\n",
    "**Accumulators should not be used inside map() functions doing so can have unintended consequences.**\n",
    "\n",
    "### Spark can rerun a task in a few instances –\n",
    "1. When a task encounters an exception, Spark will re-execute the task 4 times by default.\n",
    "2. If an executor crashes, Spark will re execute the tasks \n",
    "3. If a task is running slow, Spark can rerun another copy of the task and this is called speculative execution. It only takes results from the task which completes first.\n",
    "\n",
    "When a task re-execute, it will execute all the transformation functions in the task and this causes the accumulator value which was already manipulated by the first execution of the task to get manipulated again causing duplication in the accumulator’s result.\n",
    "\n",
    "Due to this reason, always include code related to accumulator in action functions like foreach(). Spark will not complain at compile time or runtime when you include code related to accumulator in transformation functions like map() so make sure to keep this point in mind when you deal with accumulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfda929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
