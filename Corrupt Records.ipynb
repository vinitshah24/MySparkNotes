{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd96b98",
   "metadata": {},
   "source": [
    "## Permissive Mode [DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73855f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d24a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_311'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('JAVA_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96eacb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbdeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835b845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PermissiveCorruptRec\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d609d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .schema(\"id integer, name string, join_date date, salary integer, _corrupt_record string\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .option(\"dateformat\", \"dd.MM.yyyy\") \\\n",
    "    .csv(\"data/corruptRecords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f470bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+-----------------------+\n",
      "|id |name |join_date |salary|_corrupt_record        |\n",
      "+---+-----+----------+------+-----------------------+\n",
      "|1  |John |2019-12-10|150000|null                   |\n",
      "|2  |Adam |2019-04-10|50000 |null                   |\n",
      "|3  |Sam  |2019-03-13|90000 |null                   |\n",
      "|4  |Karen|2019-03-14|null  |4,Karen,14.03.2019,100K|\n",
      "+---+-----+----------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c5e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- join_date: date (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd8b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6f76b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+--------------------+\n",
      "| id| name| join_date|salary|     _corrupt_record|\n",
      "+---+-----+----------+------+--------------------+\n",
      "|  4|Karen|2019-03-14|  null|4,Karen,14.03.201...|\n",
      "+---+-----+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_rec_df = df.filter(col(\"_corrupt_record\").isNotNull())\n",
    "corrupt_rec_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2705e3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|_corrupt_record|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_rec_df.select(col(\"_corrupt_record\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e7972",
   "metadata": {},
   "source": [
    "As we know, spark performs lazy-evaluation, i.e., actual operations on data is not performed until an action is called. In this case, when we called filter(), select() transformations, and show() action command, spark internally uses projection operation to fetch \"_corrupt_record\" data from the file where it does not exist. When we want to fetch only corrupted records, we select the \"_corrupt_record\" column and call an action show(). So, that's why we can't see that column.\n",
    "\n",
    "By caching, we can store the intermediate results in the memory, which will fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "193f7a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, join_date: date, salary: int, _corrupt_record: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CACHE IS NEEDED TO SEE THE _corrupt_record COLUMN\n",
    "corrupt_rec_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b651fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|_corrupt_record        |\n",
      "+-----------------------+\n",
      "|4,Karen,14.03.2019,100K|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrupt_rec_df.select(col(\"_corrupt_record\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deaedca",
   "metadata": {},
   "source": [
    "## DROP MALFORMED ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7619cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read \\\n",
    "    .schema(\"id integer, name string, join_date date, salary integer\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .option(\"dateformat\", \"dd.MM.yyyy\") \\\n",
    "    .csv(\"data/corruptRecords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4349b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------+\n",
      "|id |name|join_date |salary|\n",
      "+---+----+----------+------+\n",
      "|1  |John|2019-12-10|150000|\n",
      "|2  |Adam|2019-04-10|50000 |\n",
      "|3  |Sam |2019-03-13|90000 |\n",
      "+---+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8aed0",
   "metadata": {},
   "source": [
    "## FAILFAST MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb40246",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read \\\n",
    "    .schema(\"id integer, name string, join_date date, salary integer\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"dateformat\", \"dd.MM.yyyy\") \\\n",
    "    .csv(\"data/corruptRecords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "332661c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o109.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (DESKTOP-SSC2TF1 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [4,Karen,17969,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\r\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"100K\"\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n",
      "\t... 24 more\r\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"100K\"\r\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\r\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\r\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\r\n",
      "\t... 27 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [4,Karen,17969,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1764)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:69)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\r\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\t... 1 more\r\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"100K\"\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n",
      "\t... 24 more\r\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"100K\"\r\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\r\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\r\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\r\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\r\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\r\n",
      "\t... 27 more\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emp_df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ba4fe",
   "metadata": {},
   "source": [
    "## Redirect Bad Records to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b10acb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .schema(\"id integer, name string, join_date date, salary integer\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"badRecordsPath\", \"data\") \\\n",
    "    .option(\"dateformat\", \"dd.MM.yyyy\") \\\n",
    "    .csv(\"data/corruptRecords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feff54c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+\n",
      "|id |name |join_date |salary|\n",
      "+---+-----+----------+------+\n",
      "|1  |John |2019-12-10|150000|\n",
      "|2  |Adam |2019-04-10|50000 |\n",
      "|3  |Sam  |2019-03-13|90000 |\n",
      "|4  |Karen|2019-03-14|null  |\n",
      "+---+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e5706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
