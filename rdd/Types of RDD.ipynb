{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b045b8-7f68-42b6-918e-ce9c629a3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54163a-6c45-4c4c-84e3-41429540b351",
   "metadata": {},
   "source": [
    "\r\n",
    "### Summary of Key Classes:\r\n",
    "1. **RDD** – Base class for all RDDs.\r\n",
    "2. **ParallelCollectionRDD** – RDD created from in-memory collections (via `sc.parallelize`).\r\n",
    "3. **HadoopRDD** – RDD created by reading from Hadoop-based file systems (e.g., HDFS, S3).\r\n",
    "4. **UnionRDD** – RDD created by applying `union()` on two or more RDDs.\r\n",
    "5. **MapPartitionsRDD** – RDD created by transformations like `mapPartitions()`.\r\n",
    "6. **PairRDD** – RDD with key-value pairs, supporting operations like `reduceByKey()` and `groupByKey()`.\r\n",
    "7. **CoalescedRDD** – RDD created by applying `coalesce()` to reduce the number of partitions.\r\n",
    "8. **CheckpointRDD** – RDD that has been checkpointed to fault-tolerant storage.\r\n",
    "9. **WholeTextFileRDD** – RDD created by reading whole text files (using `sc.wholeTextFiles()`).\r\n",
    "10. **CachedRDD** – RDD that has been cached or persils on any of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5df69f3-515d-4067-985f-c9531293fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"rddTests\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c12d99a-89b5-48e5-bcb8-b99a6d4f8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87253f96-ecdf-43e2-9824-89417c6bf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d217298-1921-448b-8f91-ad3d6165477e",
   "metadata": {},
   "source": [
    "### **RDD** (Base Class)\n",
    "   - This is the fundamental abstraction representing a distributed collection of data in Spark.\n",
    "   - It provides methods for various transformations (like `map`, `filter`, `flatMap`) and actions (like `collect`, `count`, `saveAsTextFile`).\n",
    "   - The `RDD` class itself is generic and can be used for any distributed collection.\n",
    "\n",
    "   Example:\n",
    "   ```scala\n",
    "   val rdd = sc.parallelize(Seq(1, 2, 3))\n",
    "\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8d774-0033-424b-9708-85f06b11f7b2",
   "metadata": {},
   "source": [
    "### **HadoopRDD**\n",
    "   - This subclass is used when reading data from external storage systems such as Hadoop’s HDFS, Amazon S3, or any Hadoop-compatible file system.\n",
    "   - When you use methods like `sc.textFile()` or `sc.sequenceFile()`, Spark creates a `HadoopRDD`.\n",
    "   - It is optimized for reading large files distributed across a cluster.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val rdd = sc.textFile(\"hdfs://path/to/data\")\n",
    "   \n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdd642e-d4c5-4f51-9f16-3ce719ba8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"../data/company_data/companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d551920-c086-44a8-ad5c-60439f3580b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) ../data/company_data/companies.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  ../data/company_data/companies.csv HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533991af-b62f-4710-b2b5-1c3e919b0041",
   "metadata": {},
   "source": [
    "### **ParallelCollectionRDD**\n",
    "   - This is the subclass of `RDD` used when you create an RDD from a parallelized collection (like a list or an array) via `sc.parallelize()`.\n",
    "   - It represents an RDD that is made from an existing in-memory collection, distributed across the cluster.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val rdd = sc.parallelize(List(1, 2, 3, 4, 5))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a231f6a-6242-4d6d-8466-adb554337fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7e60e7-a948-480f-8528-b579226bc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd49985-a9d1-43f1-bc27-cb8b1caad90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_rdd = rdd.map(lambda x: x*2)\n",
    "map_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc259857-2806-410a-bbcb-58881882f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) PythonRDD[3] at collect at C:\\Users\\shahv\\AppData\\Local\\Temp\\ipykernel_10484\\1503817425.py:2 []\n",
      " |  ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(map_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06c514-ba03-4062-9ea8-5ae88543768a",
   "metadata": {},
   "source": [
    "### **MapPartitionsRDD**\n",
    "   - This subclass is created when applying a transformation like `mapPartitions()`.\n",
    "   - It operates on the data in partitions (not just individual elements) and applies the transformation to each partition as a whole, which can be more efficient than applying the transformation element-by-element.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "   val mapPartitionsRdd = rdd.mapPartitions(iterator => iterator.map(x => x * 2))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8222e8e-c715-422b-9c35-95eb7c61450d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05146c91-664a-40cb-b175-89d233977137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [9], [6], [7], [8], [19]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_partition(iterator):\n",
    "    yield sum(iterator)\n",
    "\n",
    "result_rdd = rdd.mapPartitions(sum_partition)\n",
    "result_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6242a4a-8711-4b2f-857b-0ced014c0de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) PythonRDD[7] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(result_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9063b6-fb85-424b-8163-519ef2f39265",
   "metadata": {},
   "source": [
    "### **UnionRDD**\n",
    "   - This class represents the union of two or more RDDs. It is produced when you use the `union()` transformation.\n",
    "   - It combines the data from multiple RDDs into one RDD.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val rdd1 = sc.parallelize(Seq(1, 2))\n",
    "   val rdd2 = sc.parallelize(Seq(3, 4))\n",
    "   val unionRdd = rdd1.union(rdd2)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc6b4b4-f43d-4a23-8c1f-a251d04546c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([4, 3])\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "union_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe174ed4-9222-47c1-98da-0274143f4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16) UnionRDD[10] at union at NativeMethodAccessorImpl.java:0 []\n",
      " |   ParallelCollectionRDD[8] at readRDDFromFile at PythonRDD.scala:287 []\n",
      " |   ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(union_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fb059-7c32-49ca-87a8-86b010d13504",
   "metadata": {},
   "source": [
    "### 6. **PairRDD**\n",
    "   - This class is a specialized subclass of `RDD` where each element is a key-value pair (often used with `Map` operations).\n",
    "   - PairRDDs allow for key-based transformations like `reduceByKey()`, `groupByKey()`, `join()`, and `cogroup()`.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val pairRdd = sc.parallelize(Seq((\"a\", 1), (\"b\", 2), (\"a\", 3)))\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee2348a1-2d87-4fe8-b183-bf25e8968c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('a', 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd = sc.parallelize(((\"a\", 1), (\"b\", 2), (\"a\", 3)))\n",
    "pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6abd511-ceb6-44d2-8e37-0bc4dd2986fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(pair_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1f04d-fb45-4655-a00b-3124f78310f6",
   "metadata": {},
   "source": [
    "### **CoalescedRDD**\n",
    "   - This subclass is used when applying a `coalesce()` transformation, which is designed to reduce the number of partitions in an RDD without causing a full shuffle of the data.\n",
    "   - This is especially useful when you want to optimize the number of partitions, typically for operations like writing data to disk.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val coalescedRdd = rdd.coalesce(2)\n",
    "   \n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "648a0482-9b38-4cf5-8b4a-814d9c55f73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [('a', 1)], [], [], [('b', 2)], [], [('a', 3)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(((\"a\", 1), (\"b\", 2), (\"a\", 3)))\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59bb3b4d-cbe0-4bbb-9fec-07d35e2f4d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 1)], [('b', 2), ('a', 3)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalesce_rdd = rdd.coalesce(2)\n",
    "coalesce_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8217869-f8ae-48e2-9f72-4c10433b5b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) CoalescedRDD[14] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  ParallelCollectionRDD[12] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(coalesce_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e30d56-c012-43c3-9214-2ebaf7d92f5a",
   "metadata": {},
   "source": [
    "### **CheckpointRDD**\n",
    "   - This subclass represents an RDD that has been checkpointed to storage, typically HDFS.\n",
    "   - Checkpointing helps Spark recover from failures by saving the RDD’s lineage to a fault-tolerant storage system.\n",
    "\n",
    "   Example:\n",
    "   ```scala\n",
    "   rdd.checkpoint()\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4766d5fd-b81e-456d-9b22-9b60069130de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-SSC2TF1:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rddTests</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=rddTests>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a6b4b8-0413-4a0d-9090-0e29849d7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(((\"Alice\", 1), (\"Brian\", 2), (\"Claire\", 3)))\n",
    "# rdd.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47edb25-5079-4dfc-884c-eb0edac62622",
   "metadata": {},
   "source": [
    "### **WholeTextFileRDD**\n",
    "   - This class is used when reading a directory of text files into an RDD with `sc.wholeTextFiles()`.\n",
    "   - Each element in this RDD is a tuple, where the first element is the file path, and the second is the contents of the file.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val wholeTextFileRdd = sc.wholeTextFiles(\"hdfs://path/to/directory\")\n",
    "   \n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "866dc2ad-b0d7-4dc8-80c6-46d181fea43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/D:/BigData/Spark/data/company_data/companies.csv',\n",
       "  'id,company,country_id\\r\\n1,Mybuzz,11\\r\\n2,Chatterbridge,3\\r\\n3,Skyble,7\\r\\n4,Brainverse,4\\r\\n5,Jabbertype,7\\r\\n6,Zoombeat,12\\r\\n7,Tanoodle,8\\r\\n8,Feedmix,13\\r\\n9,Meembee,20\\r\\n10,Riffpath,7\\r\\n11,Dynabox,19\\r\\n12,Browsetype,3\\r\\n13,Dynazzy,20\\r\\n14,Demizz,19\\r\\n15,Riffpedia,18\\r\\n16,Zava,13\\r\\n17,Pixonyx,20\\r\\n18,Yambee,15\\r\\n19,Yombu,7\\r\\n20,Voomm,14\\r\\n21,Skilith,12\\r\\n22,Ooba,11\\r\\n23,Oyoyo,2\\r\\n24,Avavee,3\\r\\n25,Livepath,13\\r\\n26,Meedoo,12\\r\\n27,Dynabox,13\\r\\n28,Skipfire,13\\r\\n29,Flashdog,2\\r\\n30,Twimm,12\\r\\n31,Tagfeed,14\\r\\n32,Teklist,11\\r\\n33,Tanoodle,6\\r\\n34,Linkbuzz,14\\r\\n35,Jaxbean,9\\r\\n36,Babblestorm,2\\r\\n37,Wikizz,4\\r\\n38,Quatz,5\\r\\n39,Bubbletube,12\\r\\n40,Dazzlesphere,18\\r\\n41,Centimia,17\\r\\n42,Thoughtbeat,16\\r\\n43,Roombo,17\\r\\n44,Shuffledrive,8\\r\\n45,Roodel,17\\r\\n46,Twitterworks,8\\r\\n47,Thoughtsphere,8\\r\\n48,Meejo,16\\r\\n49,Divavu,9\\r\\n50,Yamia,10\\r\\n51,Meezzy,2\\r\\n52,Thoughtmix,17\\r\\n53,Quire,20\\r\\n54,Babblestorm,13\\r\\n55,Devbug,15\\r\\n56,Meemm,4\\r\\n57,Skiba,4\\r\\n58,Riffwire,14\\r\\n59,Edgetag,4\\r\\n60,Mynte,16\\r\\n61,Innojam,12\\r\\n62,Yamia,7\\r\\n63,Quire,20\\r\\n64,Cogibox,19\\r\\n65,Buzzdog,18\\r\\n66,Rooxo,18\\r\\n67,Ailane,18\\r\\n68,Zoomcast,8\\r\\n69,Topiclounge,9\\r\\n70,Flipopia,10\\r\\n71,Yoveo,20\\r\\n72,Wikizz,20\\r\\n73,Mydeo,19\\r\\n74,Skipfire,6\\r\\n75,Zooveo,5\\r\\n76,Dabfeed,14\\r\\n77,Centimia,3\\r\\n78,Realcube,10\\r\\n79,Jaxspan,8\\r\\n80,Omba,6\\r\\n81,Feedfire,8\\r\\n82,Kanoodle,3\\r\\n83,Skyba,7\\r\\n84,Skivee,10\\r\\n85,Dabtype,13\\r\\n86,Kaymbo,9\\r\\n87,Realcube,1\\r\\n88,Feednation,9\\r\\n89,Jaxnation,7\\r\\n90,Skiba,16\\r\\n91,Skiba,9\\r\\n92,Izio,5\\r\\n93,Kwinu,11\\r\\n94,Podcat,10\\r\\n95,Zoonder,9\\r\\n96,Eire,6\\r\\n97,Yakitri,1\\r\\n98,Jaxbean,18\\r\\n99,Fadeo,5\\r\\n100,Yoveo,9')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_text_file_rdd = sc.wholeTextFiles(\"../data/company_data/companies.csv\")\n",
    "whole_text_file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fea9470b-0ebb-4f39-bcc2-d237b5775c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) ../data/company_data/companies.csv MapPartitionsRDD[17] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n",
      " |  WholeTextFileRDD[16] at wholeTextFiles at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "show_plan(whole_text_file_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d249bb1-5ec3-4e0b-9e85-65f9034ea101",
   "metadata": {},
   "source": [
    "### **CachedRDD**\n",
    "   - This is an RDD that has been persisted in memory. It’s created by calling the `cache()` or `persist()` method on an existing RDD to keep it in memory for faster access in future operations.\n",
    "   \n",
    "   Example:\n",
    "   ```scala\n",
    "   val cachedRdd = rdd.cache()\n",
    "   \n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37bbc9cb-021f-4280-8ae7-24cf91b845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(((\"Alice\", 1), (\"Brian\", 2), (\"Claire\", 3)))\n",
    "cache_rdd = rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cfcfd92-904d-4f6f-aa95-12e419024f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) ParallelCollectionRDD[18] at readRDDFromFile at PythonRDD.scala:287 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "show_plan(cache_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec01adc-9d2e-4a74-aaed-3508dee7d81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
